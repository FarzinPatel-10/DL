Assignment 3
Build the Image classification model by dividing the model.
Step 1: Loading and Preprocessing the Image Data
We’ll use TensorFlow/Keras and the ImageDataGenerator class to load and preprocess the image dataset.
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split

# Directory path to your image dataset (organized into subfolders for each class)
DATASET_PATH = 'path/to/your/dataset'

# Split parameters
IMAGE_SIZE = (128, 128)  # Resize all images to 128x128
BATCH_SIZE = 32  # Number of images processed together
SEED = 42  # Ensure reproducibility

# Create ImageDataGenerator objects for data augmentation
datagen = ImageDataGenerator(
    rescale=1.0/255.0,  # Normalize pixel values between 0 and 1
    validation_split=0.2,  # Use 20% of data for validation
    rotation_range=15,  # Randomly rotate images
    width_shift_range=0.1,  # Shift width randomly
    height_shift_range=0.1,  # Shift height randomly
    horizontal_flip=True  # Random horizontal flip
)

# Load and split data into training and validation sets
train_data = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',  # Use 'categorical' for multi-class classification
    subset='training',
    seed=SEED
)

val_data = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    seed=SEED
)

ImageDataGenerator: Helps in loading and augmenting the dataset.
Data Augmentation: Adds randomness (rotation, shifts, flips) to prevent overfitting.
Normalization: We rescale pixel values to be between 0 and 1.
Train-Validation Split: 80% data for training, 20% for validation to check model generalization.

Step 2: Defining the Model’s Architecture
We’ll build a Convolutional Neural Network (CNN) for image classification.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Build the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),  # 1st Convolutional layer
    MaxPooling2D(pool_size=(2, 2)),  # Max Pooling

    Conv2D(64, (3, 3), activation='relu'),  # 2nd Convolutional layer
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu'),  # 3rd Convolutional layer
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),  # Flatten feature maps to 1D vector
    Dense(128, activation='relu'),  # Fully connected layer
    Dropout(0.5),  # Dropout for regularization
    Dense(train_data.num_classes, activation='softmax')  # Output layer with softmax
])

# Compile the model
model.compile(
    optimizer='adam',  # Adam optimizer for faster convergence
    loss='categorical_crossentropy',  # Suitable loss for multi-class classification
    metrics=['accuracy']
)

# Summary of the model
model.summary()

 Convolutional Layers: Extract features from input images.
 MaxPooling: Reduces spatial dimensions to prevent overfitting.
 Flatten Layer: Converts feature maps to a 1D vector for Dense layers.
 Dropout: Prevents overfitting by randomly disabling some neurons.
 Softmax Layer: Gives probabilities for each class.
 Adam Optimizer: Adaptive optimizer for efficient training.


Step 3: Training the Model
We’ll train the model using the training and validation data generators.

# Train the model
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=10,  # Number of epochs
    verbose=1  # Display training progress
)

 Epochs: Controls how many times the model sees the entire dataset.
 Training Data: The model learns from this dataset.
 Validation Data: Used to evaluate the model after each epoch.

Step 4: Estimating the Model’s Performance
After training, we’ll evaluate the model on the validation data.

import matplotlib.pyplot as plt

# Evaluate the model on validation data
val_loss, val_accuracy = model.evaluate(val_data)
print(f"Validation Loss: {val_loss:.4f}")
print(f"Validation Accuracy: {val_accuracy:.4f}")

# Plot training history (accuracy and loss)
def plot_history(history):
    plt.figure(figsize=(12, 4))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Call the function to plot training history
plot_history(history)

 Model Evaluation: We compute loss and accuracy on the validation dataset.
 Training History Plot: Visualizes how the model’s accuracy and loss change over epochs.

